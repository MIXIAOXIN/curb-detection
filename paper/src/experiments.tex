In order to evaluate and validate the approach proposed in this paper, we have
conducted experiments on simulated and real-world data. Real-world data
has been acquired with a static nodding Laser Range-Finder (LRF) setup. Two
different lasers have been mounted, namely a SICK LMS-200 and an Hokuyo
UTM-30LX. We also tested our method on a pedestrian robot equipped with a
downward-facing SICK LMS-151 LRF that generates 3D point cloud while moving.
Simulated data has been generated by sampling from known mixture models and from
Morsel, a 3D mobile robot simulator developed in our lab.

\subsection{Experimental Conditions and Quantitative Measures}

For the nodding lasers setup, we have recorded 33 3D point clouds with multiple
viewpoints from a standard street scene. For the pedestrian robot scenario, data
has been generated from a tour in a city center and from a drive in our
3D simulator using a city-like environment (Fig.\ref{fig:morsel}).

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig/morsel}
\caption{Our pedestrian robot driving in Morsel in the city-like environment
used for evaluation.}
\label{fig:morsel}
\end{figure}

A quantitative evaluation of our algorithm can be carried out under various
interrelated perspectives: curb location in $x,y$, curb height in $z$,
number of planes, assignment of DEM cells to planes, plane parameters, or
computation time. Since we do not have ground truth information for the plane
parameters, we evaluated the predictive accuracy of a model trained
with one point cloud to the others in a similar fashion as~\cite{faria10fitting}.
To this end, we collected 19 point clouds from the same position and performed
cross-validation. Concretely, we iteratively estimate the parameter set
$\{\hat{\Theta},\mathbf{\hat{L}}\}$ using one point cloud and evaluate the
predictive error on the 18 remaining ones. The quantitative measure is the Root
Mean Square (RMS) error of prediction.

%\begin{equation}
%\label{eqn:rmspred}
%RMSEP=\sqrt{\frac{1}{M}\sum_{i=1}^M(\hat{\mu}_{h_i}-\mathbf{\hat{w}}_k^\text{T}
%\boldsymbol{\phi}(\mathbf{c}_i))^ 2)},
%\end{equation}

%where $M$ is the number of valid cells in the point cloud being predicted and
%$\mathbf{\hat{w}}_k$ corresponds to the trained component at the MAP state of
%the CRF at $\mathbf{c}_i$.

To analyze the quality of the segmentation, we manually labeled the point clouds
in regions corresponding to plane segments. While accurate ground truth is
available from the simulator, real data labeling might suffer from slight
subjective errors. Since ground truth and inferred labels can differ in an
unsupervised clustering framework, we have used the
V-Measure~\cite{rosenberg07vmeasure} as a quantitative measure. This conditional
entropy-based figure ranging from 0 (bad) to 1 (good) combines homogeneity and
completeness criteria, and copes with labels mismatches.

In order to further analyze our model, we have sampled point clouds from known
mixture of linear regressions and also evaluated in this case the RMS
error of the predicted parameters $\hat{\Theta}$ against their ground truth. In
the case of synthetic data, predicted curb location and height, and assignment
of cells to planes, can also be quantitatively evaluated. Furthermore, we can
judge the robustness and validity of our algorithm on various situations such
as T junctions, inclined planes, or lowered curbs.

\subsection{Qualitative Evaluation}
Before we proceed with the actual quantitative analysis, we want to give a
glimpse on some qualitative results that demonstrate the pertinence of our
approach.

In Fig.~\ref{fig:europa}, our pedestrian robot navigates in a city center and
labels curbs while driving. Since the point cloud is reconstructed while the
robot drives, curbs can only be detected behind the robot in this specific
situation. DEM patches are labeled sequentially and we achieve on-line and
real-time performance.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig/europa.eps}
\caption{Example of curb detection from a moving pedestrian robot. Colors
represent planes and curbs are located at their boundaries.}
\label{fig:europa}
\end{figure}

Fig.~\ref{fig:special} depicts a situation that a pedestrian robot might often
encounter when crossing a street. Using sampled data, we demonstrate in
Fig.~\ref{fig:complex} the output of our algorithm in a complex environment
containing a T junction, inclined planes, and lowered curbs. These experiments
illustrate that our method can cope with multiple viewpoints and environment
configurations. Most of the competitive approaches would fail in that cases.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig/special.eps}
\caption{Example of curb detection in an unfavorable situation. Our algorithm
correctly label the planes, and thus curbs, under various viewpoints and
experimental settings.}
\label{fig:special}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig/sampled.eps}
\caption{As exemplified on synthetic data, our method can successfully cope with
various environment configurations.}
\label{fig:complex}
\end{figure}

Fig.~\ref{fig:segment} displays the outcome of the segmentation algorithm on a
point cloud, while Fig.~\ref{fig:ml} shows the results obtained from the
standard EM algorithm. As a comparison, our method is applied on the same data
and the result is depicted in Fig.~\ref{fig:crf-em}. These experiments clearly
highlight the advantages of our method.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig/segment.eps}
\caption{Graph-based segmentation of the DEM (colors encode assignments of
cells to the different components). By keeping the segmentation parameter $s$
rather low, we ensure no planes are missed, at the cost of over-segmentation.}
\label{fig:segment}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig/ml.eps}
\caption{DEM cell assignments (colors) to plane components using the standard
EM algorithm. Due to the initial over-segmentation, the algorithm is not able
to fully smooth out the additional planes.}
\label{fig:ml}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{fig/crf-em.eps}
\caption{DEM cell assignments (colors) using our CRF-EM implementation. The MAP
state is displayed here. The algorithm is able to smooth out the unnecessary
planes and provides a good estimate of the curb positions and heights.}
\label{fig:crf-em}
\end{figure}

\subsection{Quantitative Evaluation and Discussion}

\subsubsection{Real-World Data}

Proceeding as mentioned above, i.e., evaluating the prediction accuracy on
unseen data and averaging the RMSE over all the datasets, we obtain a RMSE of
0.016 [m] for a DEM containing 1600 cells. This result shows that we are able
to accurately estimate the plane parameters and thus reconstruct the scene.

Under the same experimental conditions, the quality of the plane segmentation
has been quantitatively evaluated on all the datasets and we obtained a
V-Measure of 0.96 on average with a standard deviation of 0.03. We have noticed
here that the badly classified cells are mostly due to grid discretization
or subjective ground truth.

Our algorithm takes, on a standard dual-core laptop, on average 0.21 [s] with
a standard deviation of 0.17. As expected, most of the computation time is spent
on the BP-EM part. These timings therefore allow for real-time and on-line
operations on a robotic platform.

\subsubsection{Synthetic Data}

For the first experiment in our 3D mobile robot simulator, we have evaluated the
segmentation quality and obtained a V-Measure of 0.88 on average with a standard
deviation of 0.05. Here again, we have witnessed that the discretization induces
the badly classified cells.

In a second experiment, we have generated multiple artificial mixture of linear
regression models and sampled 3D point clouds from them. We tried to create
situations that were not present in our dataset and varied the different
parameters to analyze the robustness of our algorithm. We have evaluated and
validated our algorithm under the aforementioned criteria.
